{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization of RoI using Image Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pywt \n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity\n",
    "from skimage.measure import shannon_entropy\n",
    "import flirimageextractor\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Domain Techniques for Image Fusion\n",
    "\n",
    "    Spatial techniques directly acting on the image pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Summation Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha represents weight of first image\n",
    "# set alpha = 0.5 for unweighted average\n",
    "\n",
    "def weightedFusion(image1, image2, alpha):\n",
    "    \n",
    "    # simple weighted average of images\n",
    "    \n",
    "    dst = cv.addWeighted(image1, alpha, image2, 1 - alpha, 0.0)\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Fusion ( Principal Component Analysis )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding weights by feature extraction using PCA \n",
    "\n",
    "def pcaFusion(image1, image2):\n",
    "    \n",
    "    sz = image1.shape\n",
    "\n",
    "    data = np.zeros((sz[0]*sz[1]*sz[2], 2))\n",
    "    data[:, 0] = image1.flatten()\n",
    "    data[:, 1] = image2.flatten()\n",
    "\n",
    "    # calculating appropriate weights for images using eigenvectors\n",
    "\n",
    "    _, eigenvectors, eigenvalues = cv.PCACompute2(data, mean=None)\n",
    "    if(eigenvalues[0][0] > eigenvalues[1][0]):\n",
    "        eg = 0\n",
    "    else:\n",
    "        eg = 1\n",
    "        \n",
    "    ref = abs(eigenvectors[eg])\n",
    "    s = np.sum(ref)\n",
    "    \n",
    "    ref = [ref[0]/s, ref[1]/s]\n",
    "    dst = (ref[0] * image1 + ref[1] * image2)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HSI Fusion ( Hue, Saturation, Intensity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha represents weight of first image\n",
    "\n",
    "def hsvFusion(image1, image2, alpha):\n",
    "\n",
    "    tmp1 = cv.cvtColor(image1, cv.COLOR_BGR2GRAY, dstCn = 1)\n",
    "    tmp2 = cv.cvtColor(image2, cv.COLOR_BGR2GRAY, dstCn = 1)\n",
    "\n",
    "    # converting thermal image to HSV for extraction of color map\n",
    "    \n",
    "    hsv_image = cv.cvtColor(image2, cv.COLOR_BGR2HSV)\n",
    "    hsv_channels = cv.split(hsv_image)\n",
    "\n",
    "    # adding image intensities (grayscale images) using weights \n",
    "\n",
    "    res_channel = cv.addWeighted(tmp1, alpha, tmp2, 1-alpha, 0.0)\n",
    "\n",
    "    # replacing intensity channel of thermal candidate to weightd channel\n",
    "\n",
    "    dst = cv.merge([hsv_channels[0], hsv_channels[1], res_channel])\n",
    "    dst = cv.cvtColor(dst, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Domain Techniques for Image Fusion\n",
    "\n",
    "    Techniques based on signal frequency of pixels (for extraction of features from candidate images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian pyramid for merging and fusion of images\n",
    "\n",
    "# generate Gaussian pyramid (pre-emtive step for Lplacian)\n",
    "\n",
    "def generateGaussian(img, n):\n",
    "\n",
    "    gD = img.copy()\n",
    "    gPimg = [gD]\n",
    "    for i in range(n):\n",
    "        gD = cv.pyrDown(gD)\n",
    "        gPimg.append(gD)\n",
    "    \n",
    "    return gPimg\n",
    "\n",
    "# generate Laplacian pyramid from gaussian inputs\n",
    "\n",
    "def generateLaplace(gPimg, n):\n",
    "    \n",
    "    lPimg = [gPimg[n]]\n",
    "    for i in range(n, 0, -1):\n",
    "        r,c,ch = gPimg[i-1].shape\n",
    "\n",
    "        gU = cv.pyrUp(gPimg[i])\n",
    "        gU = gU[:r, :c]\n",
    "        \n",
    "        lP = cv.subtract(gPimg[i-1], gU)\n",
    "        lPimg.append(lP)\n",
    "    \n",
    "    return lPimg\n",
    "\n",
    "# function to fuse laplace outputs layer-wise while going up to get desired results\n",
    "\n",
    "def weightedAvg(lPA, lPB, alpha):\n",
    "    \n",
    "    LS = []\n",
    "    \n",
    "    # weighted averaging for each pyramid layer \n",
    "    \n",
    "    for la, lb in zip(lPA, lPB):\n",
    "        ls = cv.addWeighted(la, alpha, lb, 1-alpha, 0.0)\n",
    "        LS.append(ls)\n",
    "    \n",
    "    res = LS[0]\n",
    "    for i in range(1, len(lPA)):\n",
    "        r,c,ch = LS[i].shape\n",
    "\n",
    "        res = cv.pyrUp(res)\n",
    "        res = res[:r, :c]\n",
    "\n",
    "        res = cv.add(res, LS[i])\n",
    "\n",
    "    return res\n",
    "\n",
    "# calling laplace fusion here\n",
    "\n",
    "def laplaceFusion(image1, image2, alpha, n):\n",
    "    \n",
    "    gPA = generateGaussian(image1, n)\n",
    "    gPB = generateGaussian(image2, n)\n",
    "\n",
    "    lPA = generateLaplace(gPA, n-1)\n",
    "    lPB = generateLaplace(gPB, n-1)\n",
    "\n",
    "    dst = weightedAvg(lPA, lPB, alpha)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCT Fusion ( Discrete Cosine Transforms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete Cosine/Fourier tranform for fusion of images\n",
    "\n",
    "def dctFusion(image1, image2, alpha):\n",
    "\n",
    "    # channel-wie cosine transforms for input images\n",
    "    \n",
    "    b1, g1, r1 = cv.split(image1)\n",
    "    tmpB1, tmpG1, tmpR1 = cv.dct(np.float32(b1)), cv.dct(np.float32(g1)), cv.dct(np.float32(r1))\n",
    "\n",
    "    b2, g2, r2 = cv.split(image2)\n",
    "    tmpB2, tmpG2, tmpR2 = cv.dct(np.float32(b2)), cv.dct(np.float32(g2)), cv.dct(np.float32(r2))\n",
    "    \n",
    "    # inverse cosine transforms to regenerate image after weighted sum\n",
    "\n",
    "    B = cv.idct(cv.addWeighted(tmpB1, alpha, tmpB2, 1-alpha, 0.0))\n",
    "    G = cv.idct(cv.addWeighted(tmpG1, alpha, tmpG2, 1-alpha, 0.0))\n",
    "    R = cv.idct(cv.addWeighted(tmpR1, alpha, tmpR2, 1-alpha, 0.0))\n",
    "    \n",
    "    dst = cv.merge((B, G, R))\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DWT Fusion ( Discrete Wavelet Transforms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Wavelet tranform for fusion (without masking)\n",
    "\n",
    "def preDWT(image):\n",
    "    \n",
    "    b, g, r = cv.split(image)\n",
    "\n",
    "    # calculating dwt coefficients for each channel\n",
    "\n",
    "    coeffB, coeffG, coeffR = pywt.dwt2(b, 'bior1.3'), pywt.dwt2(g, 'bior1.3'), pywt.dwt2(r, 'bior1.3')\n",
    "    return [coeffB, coeffG, coeffR]\n",
    "\n",
    "def dwtFusion(image1, image2, alpha):\n",
    "    \n",
    "    arr1 = preDWT(image1)\n",
    "    arr2 = preDWT(image2)\n",
    "    \n",
    "    arr = []\n",
    "    for coeff1, coeff2 in zip(arr1, arr2):\n",
    "\n",
    "        # weighted sum of approximations of images followed by maxima technique for DWT coefficients\n",
    "\n",
    "        cA = cv.addWeighted(coeff1[0], alpha, coeff2[0], 1-alpha, 0.0)\n",
    "        cD = []\n",
    "        for cD1, cD2 in zip(coeff1[1], coeff2[1]):\n",
    "            cD.append(np.maximum(cD1, cD2))\n",
    "        arr.append([cA, cD])\n",
    "    \n",
    "    # inverse wavelet transforms to regenerate image after weighted sum\n",
    "    res = []\n",
    "    for coeff in arr:\n",
    "        d = pywt.idwt2(coeff, 'haar')\n",
    "        res.append(d)\n",
    "    \n",
    "    dst = cv.merge(res)\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Segmentation\n",
    "\n",
    "    Segmentation functions to segment region based on thresholding outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptThreshold(image):\n",
    "\n",
    "    # Adaptive thresholding to generate binary output\n",
    "\n",
    "    img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    th = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 13, 2)\n",
    "    th = cv.bitwise_not(th)\n",
    "\n",
    "    # finding contours and returning coordinates of bounding rectangle\n",
    "\n",
    "    contours, _ = cv.findContours(th, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv.contourArea)\n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "    \n",
    "    ## to show bounding rectangle of obtained contour on input image, uncomment following lines\n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('adaptive-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def otsuThreshold(image):\n",
    "\n",
    "    # Otsu thresholding to generate binary output\n",
    "\n",
    "    img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    _, th = cv.threshold(img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "    # finding contours and returning coordinates of bounding rectangle\n",
    "\n",
    "    contours, _ = cv.findContours(th, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv.contourArea)\n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "\n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines \n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('otsu-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x,y,w,h "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted Otsu Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertOtsuThreshold(image):\n",
    "    \n",
    "    # inverted Otsu thresholding (black -> white, white -> black)\n",
    "    # inverse of otsuThreshold()\n",
    "    \n",
    "    img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    _, th = cv.threshold(img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    th = cv.bitwise_not(th)\n",
    "\n",
    "    contours, _ = cv.findContours(th, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv.contourArea)\n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "\n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines \n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('inverted_otsu-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge-based Segmentation\n",
    "\n",
    "   Manually tuned canny thresholds for edge detection and segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canny edge detection for fused image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny(image, thres1, thres2):\n",
    "\n",
    "    # finding canny edge\n",
    "\n",
    "    edges = cv.Canny(image, thres1, thres2)\n",
    "\n",
    "    # finding contours and returning coordinates of bounding rectangle\n",
    "\n",
    "    contours, _ = cv.findContours(edges, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv.contourArea)\n",
    "    \n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "\n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines\n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('canny-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canny edge detection for best output from fused / visual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genFocus(image, vis, thres1, thres2):\n",
    "    \n",
    "    edges1 = cv.Canny(image, thres1, thres2)\n",
    "    edges2 = cv.Canny(vis, thres1, thres2)  \n",
    "    \n",
    "    contours1, _ = cv.findContours(edges1, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours2, _ = cv.findContours(edges2, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    coord1, coord2 = (0,0,0,0), (0,0,0,0)\n",
    "    \n",
    "    if len(contours1) != 0:\n",
    "        cnt1 = max(contours1, key = cv.contourArea)\n",
    "        coord1 = cv.boundingRect(cnt1)\n",
    "    if len(contours2) != 0:\n",
    "        cnt2 = max(contours2, key = cv.contourArea)\n",
    "        coord2 = cv.boundingRect(cnt2)\n",
    "\n",
    "    x,y,w,h = max([coord1, coord2], key = lambda k : k[2]*k[3])\n",
    "\n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines\n",
    "    \n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x, y), (x+w, y+h), (0, 255, 0), 1)  \n",
    "\n",
    "    # cv.imshow('dst', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x, y, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Canny Segmentation\n",
    "\n",
    "    Using Otsu thresholds as minimum and maximum canny limits for edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CannyWithOtsu(image):\n",
    "\n",
    "    # Otsu thresholding to find threshold metric\n",
    "\n",
    "    img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    res, _ = cv.threshold(img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "    # minimum threshold : 0.5*OT, maximum thresholds: OT\n",
    "\n",
    "    edges = cv.Canny(img, res*0.5, res)\n",
    "\n",
    "    # finding contours and returning coordinates of bounding rectangle\n",
    "\n",
    "    contours, _ = cv.findContours(edges, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv.contourArea)\n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "    \n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines\n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('canny_otsu-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Segmentation\n",
    "\n",
    "    Morphological Segmentation using Watershed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watershed(image):\n",
    "\n",
    "    # Proprocessing steps to define markers for watershed\n",
    "    #  thresholding using Otsu\n",
    "    \n",
    "    gray = cv.cvtColor(image,cv.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv.threshold(gray,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
    "\n",
    "    # Morphological opening to reduce noise\n",
    "\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    opening = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations = 2)\n",
    "\n",
    "    # marking sure foreground and background \n",
    "\n",
    "    sure_bg = cv.dilate(opening,kernel,iterations=3)\n",
    "    \n",
    "    # distance tranform to find centroids of foreground objects\n",
    "\n",
    "    dist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "    _, sure_fg = cv.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv.subtract(sure_bg,sure_fg)\n",
    "\n",
    "    # defining markers on foreground using connected components\n",
    "\n",
    "    _, markers = cv.connectedComponents(sure_fg)\n",
    "    markers = markers+1\n",
    "    markers[unknown==255] = 0\n",
    "\n",
    "    # using markers for watershed\n",
    "\n",
    "    markers = cv.watershed(image, markers)\n",
    "\n",
    "    ## ! to show bounding foreground detected by watershed, uncomment following lines\n",
    "    \n",
    "    # copy = image.copy()\n",
    "    # copy[markers == -1] = [0,255,0]\n",
    "    \n",
    "    # cv.imshow('watershed-foreground', copy)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Segmentation\n",
    "    \n",
    "    FCM and K-means algorithm for color-based segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcm(image, C):\n",
    "    temp = image.copy()\n",
    "    tmp = temp.reshape((-1, 3)).T\n",
    "\n",
    "    # using skmeans fcm to get cluster centers and ditribution\n",
    "\n",
    "    centers, u, _, _, _, _, _ = fuzz.cluster.cmeans(tmp, c=C, m=2, error=0.005, maxiter=1000)\n",
    "\n",
    "    img_clustered = np.argmax(u, axis=0).astype(int)\n",
    "    cluster2D = img_clustered.reshape((image.shape[0], image.shape[1]))\n",
    "    \n",
    "    # changing distribution in accordance to center labels\n",
    "\n",
    "    res = np.zeros(image.shape)\n",
    "    for i in range(cluster2D.shape[0]):\n",
    "        for j in range(cluster2D.shape[1]):\n",
    "            k = cluster2D[i][j]\n",
    "            res[i][j] = centers[k]\n",
    "    \n",
    "    ## ! to show color segmented output, uncomment following lines \n",
    "\n",
    "    # cv.imshow('color-cluster', res)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return res.astype(np.uint8)\n",
    "\n",
    "def kmeans(image, K):\n",
    "    \n",
    "    temp = image.copy()\n",
    "    tmp = np.float32(temp.reshape((-1, 3)))\n",
    "    \n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "\n",
    "    # using opencv kmeans to get cluster labels and ditribution\n",
    "\n",
    "    _, label, center = cv.kmeans(tmp, K, None, criteria, 10, cv.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    center = np.uint8(center)\n",
    "    res = center[label.flatten()].reshape((temp.shape))\n",
    "\n",
    "    ## ! to show color segmented output, uncomment following lines \n",
    "    \n",
    "    # cv.imshow('color-cluster', res)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "    Main functions for Localization of ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color segregation step (for layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorSeg(image, base, vals):\n",
    "    image_hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
    "    h, _, _ = cv.split(image_hsv)\n",
    "\n",
    "    bins = np.bincount(h.flatten())\n",
    "    peaks = np.where(bins > 0)[0]\n",
    "    \n",
    "    ## ! to see the hue values detected in the image, uncomment `peaks`\n",
    "    # print(peaks)\n",
    "    \n",
    "    m,n = vals\n",
    "    pres = False\n",
    "    for i in peaks:\n",
    "        if i>=m and i<=n:\n",
    "            pres = True\n",
    "            break\n",
    "    \n",
    "    if pres:\n",
    "        mask = cv.inRange(h, m, n)\n",
    "    else:\n",
    "        mask = cv.inRange(h, int(peaks[0]), int(peaks[0]))\n",
    "\n",
    "    ## ! to show filtered hue range from segmented inputs, uncomment following lines  \n",
    "\n",
    "    # blob = cv.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    # cv.imshow('filtered-hue', blob)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key=cv.contourArea)\n",
    "    x,y,w,h = cv.boundingRect(cnt)\n",
    "    \n",
    "    ## ! to show bounding rectangle of obtained contour on input image, uncomment following lines\n",
    "\n",
    "    # copy = image.copy()\n",
    "    # t = cv.rectangle(copy, (x,y), (x+w, y+h), (0,255,0), 1)\n",
    "\n",
    "    # cv.imshow('canny_otsu-contour-bound', t)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return x, y, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main focus() function to localize RoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focus(image, vis, sz, K, iter, vals, index):\n",
    "\n",
    "    if(iter == 0):\n",
    "\n",
    "        ## ! Last step segmentation can be changed here.... \n",
    "        ## ! change function below to get coordinates\n",
    "     \n",
    "        return CannyWithOtsu(image)\n",
    "    \n",
    "    if(sz[1] != 0):\n",
    "        temp = image[:, sz[0]:sz[1]]\n",
    "        vis_temp = vis[:, sz[0]:sz[1]]\n",
    "    else:\n",
    "        temp = image.copy()\n",
    "        vis_temp = vis.copy()\n",
    "        \n",
    "    res = kmeans(temp, K)\n",
    "    base_X, base_Y, ext_X, ext_Y = colorSeg(res, sz[0], vals[iter-1])\n",
    "\n",
    "    # resizing image to localize area obtained from color segmentation \n",
    "\n",
    "    t = temp[base_Y:base_Y + ext_Y, base_X:base_X + ext_X]\n",
    "    v = vis_temp[base_Y:base_Y + ext_Y, base_X:base_X + ext_X].astype(np.uint8)\n",
    "\n",
    "    # recursive call to focus, depending on number of layers chosen for color segmentation\n",
    "\n",
    "    x, y, w, h = focus(t, v, [0,0], K, iter-1, vals, index)\n",
    "\n",
    "    return base_X + x, base_Y + y, w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask Obtained Localization with black background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskROI(coord, image):\n",
    "    \n",
    "    x1,y1,x2,y2 = coord\n",
    "    mask = np.zeros(image.shape[:2], dtype=\"uint8\")\n",
    "    \n",
    "    cv.rectangle(mask, (x1,y1), (x2,y2), 255, -1)\n",
    "    masked = cv.bitwise_and(image, image, mask=mask)\n",
    "    \n",
    "    ## ! to show masked localized region\n",
    "\n",
    "    # cv.imshow('res', masked)\n",
    "    # cv.waitKey(0)\n",
    "    # cv.destroyAllWindows()\n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "    Evaluation Metrics for localization and fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection of global truth and obtained localization\n",
    "\n",
    "def intersection(a, b):\n",
    "    \n",
    "    x = max(a[0], b[0])\n",
    "    y = max(a[1], b[1])\n",
    "    w = min(a[2], b[2]) - x\n",
    "    h = min(a[3], b[3]) - y\n",
    "    \n",
    "    if w<0 or h<0: \n",
    "        return 0\n",
    "\n",
    "    return w*h\n",
    "\n",
    "# union of global truth and localized region\n",
    "\n",
    "def union(a, b):\n",
    "    \n",
    "    x = min(a[0], b[0])\n",
    "    y = min(a[1], b[1])\n",
    "    w = max(a[2], b[2]) - x\n",
    "    h = max(a[3], b[3]) - y\n",
    "    \n",
    "    return w*h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics to assess fusion techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak signal to noise ratio\n",
    "\n",
    "def PSNR(original, fused):\n",
    "\n",
    "    original = cv.resize(original, (fused.shape[1], fused.shape[0]))\n",
    "    \n",
    "    return peak_signal_noise_ratio(original, fused)\n",
    "\n",
    "# mean squared error\n",
    "\n",
    "def MSE(original, fused):\n",
    "\n",
    "    original = cv.resize(original, (fused.shape[1], fused.shape[0]))\n",
    "    \n",
    "    return mean_squared_error(original, fused)\n",
    "\n",
    "# structural similarity\n",
    "\n",
    "def SSIM(original, fused):\n",
    "    \n",
    "    original = cv.resize(original, (fused.shape[1], fused.shape[0]))\n",
    "\n",
    "    original = cv.cvtColor(original, cv.COLOR_BGR2GRAY)\n",
    "    fused = cv.cvtColor(fused, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    score, _ = structural_similarity(original, fused, full=True)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics to assess localization procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shannon entropy of localized region\n",
    "\n",
    "def ENT(coord, fused):\n",
    "    \n",
    "    x1, y1, x2, y2 = coord\n",
    "    \n",
    "    return ('Entropy: ' + str(shannon_entropy(fused[y1:y2, x1:x2])))\n",
    "\n",
    "# Mean of temperatures (from thermal image) of localized region\n",
    "\n",
    "def thermal_mean(coord, path):\n",
    "    \n",
    "    x1, y1, x2, y2 = coord\n",
    "\n",
    "    flir = flirimageextractor.FlirImageExtractor()\n",
    "    flir.process_image(path)\n",
    "\n",
    "    return ('Mean temperature: ' + str(np.mean(flir.get_thermal_np()[y1:y2, x1:x2])))\n",
    "\n",
    "# Area of localized region\n",
    "\n",
    "def area(coord):\n",
    "\n",
    "    x1,y1,x2,y2 = coord\n",
    "    \n",
    "    return ('Area: ' +  str((x2-x1)*(y2-y1)))\n",
    "\n",
    "# Euclidean distance between centers of localized region and ground truth\n",
    "\n",
    "def dist(coord, center):\n",
    "    \n",
    "    x1, y1, x2, y2 = coord\n",
    "\n",
    "    obtX, obtY = (x1+x2)/2, (y1+y2)/2\n",
    "    cenX, cenY = center\n",
    "\n",
    "    return ('Centroid Distance: ' + str(sqrt((obtX - cenX)**2 + (obtY - cenY)**2)))\n",
    "\n",
    "# Intersection over union (overlapping region metric) for localized region and ground truth\n",
    " \n",
    "def IOU(coord1, coord2):\n",
    "    \n",
    "    return ('mAP: ' + str(intersection(coord1, coord2) / union(coord1, coord2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set evaluation metrics here\n",
    "\n",
    "def evaluate(image, coord, path, center, index, gt):\n",
    "\n",
    "    ## ! uncomment to save metrics to file\n",
    "    \n",
    "    # file = <set file path> \n",
    "    # with open(file, \"a+\") as f:\n",
    "    #     f.write('#### ' + str(index) + ' ###\\n' + ENT(gt, image) + '\\n' + thermal_mean(gt, path) + '\\n' + area(gt) + '\\n' + dist(coord, center) + '\\n' + IOU(coord, gt) + '\\n')\n",
    "    \n",
    "    print(str(index) + ':: ' + IOU(coord, gt))\n",
    "    print(str(index) + ':: ' + dist(coord, center))\n",
    "    print(str(index) + ':: ' + area(coord))\n",
    "    print(str(index) + ':: ' + thermal_mean(coord, path))\n",
    "    print(str(index) + ':: ' + ENT(coord, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing \n",
    "\n",
    "    Input images and calling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImgProcess(image1, image2, path, center, r, index, gt):\n",
    "\n",
    "    ########## Fusion Process ############\n",
    "\n",
    "    ## ! set fusion function here \n",
    "\n",
    "    fused = hsvFusion(image1, image2, 0.7)\n",
    "    fused = cv.resize(fused, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "    ## ! save fusion results to folder\n",
    "    # p = <directory_path> + str(index) + '.jpg'\n",
    "    \n",
    "    # cv.imwrite(p, dst)\n",
    "\n",
    "    ###########   Localization process ###########\n",
    "\n",
    "    # define parameters here\n",
    "    # k -> clustering parameter\n",
    "    # vals -> hue ranges for layers\n",
    "    # len(vals) = number of layers, .... 3 here\n",
    "\n",
    "    K = 6\n",
    "    vals = [ (0, 10), (0, 15), (0, 20) ]\n",
    "\n",
    "    # calling focus function\n",
    "    # focus( fused_image,  visual_image,  crop_coordinates_for_x-axis,  K-value_of_sgmentation,  number_of_layers,  hue_ranges_for_layers,  image_index )\n",
    "\n",
    "    x,y,w,h = focus(fused, image1, r, 6, 3, vals, index)\n",
    "    x1, y1, x2, y2 = x+r[0], y, x+r[0]+w, y+h\n",
    "    coords = (x1,y1,x2,y2)\n",
    "    \n",
    "    RoI = fused.copy()\n",
    "    cv.rectangle(RoI, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "\n",
    "    ## ! save localization results to folder\n",
    "    # p = <directory_path> + str(index) + '.jpg'\n",
    "    \n",
    "    # cv.imwrite(p, copy)\n",
    "\n",
    "    ########### Results #################\n",
    "\n",
    "    ## ! show fused image result here\n",
    "\n",
    "    cv.imshow('Fused image', fused)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    ## ! show localized region result here\n",
    "\n",
    "    cv.imshow('Localized region', RoI)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    fused = cv.imread('fused.jpg')\n",
    "\n",
    "    ############# Evaluate results ####################\n",
    "\n",
    "    # evaluate( fused_image,  RoI_coordinates,  thermal_image_path,  center_of_global_truth,  image_index,  global_truth_coordinates )\n",
    "\n",
    "    evaluate(fused, coords, path, center, index, gt)\n",
    "    \n",
    "    ## ! return fused and RoI images for further computation\n",
    "    \n",
    "    # return fused, RoI\n",
    "\n",
    "def ReadImg(index, centers, crop, GT):\n",
    "\n",
    "    # set visual and thermal images here (from path)\n",
    "    Tpath = './Assets/fusion/Thermal/' + str(index)+ '.jpg'\n",
    "    Vpath = './Assets/fusion/Visual/' + str(index)+ '.jpg'\n",
    "\n",
    "    img = cv.imread(Vpath)\n",
    "    Timg = cv.imread(Tpath)\n",
    "\n",
    "    # ImgProcess( visual_image,  thermal_image,  thermal_image_path,  image_center,  image_crop,  image_index,  global_truth )\n",
    "\n",
    "    return ImgProcess(img, Timg, Tpath, centers[index-1], crop[index-1], index, GT[index-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually calculated Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop coordinates for X-axis (for input images)\n",
    "\n",
    "crop = [[80,240], [80,240], [80,240], [80,240], [0, 150], [80,240], [80,240], [100,250], [80,240], [100,260], [80,240], [100,250]]\n",
    "\n",
    "# global truth coordinates\n",
    "# (0,0,1,1) for images with irregular global truths\n",
    "\n",
    "centers = [(162, 65), (190, 60), (200, 60), (142, 124), (87, 63), (197, 52), (137, 74), (215, 70), (0, 0), (209, 86), (160, 62), (212, 153)]\n",
    "GT = [(148,54,172,75), (0,0,1,1), (190,40,213,62), (127, 114,153,140), (67,48,92,80), (180,41,208,74), (0,0,1,1), (207,67,222,85), (0,0,1,1), (195,68,220,100), (148,52,172,73), (207,139,218,161)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caller function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- images are stored as str(index).jpg ..... change in ReadImg for different custom path\n",
    "## ! for looped results based on index, uncomment\n",
    "\n",
    "# for i in range(1, len(GT)+1):\n",
    "    # ReadImg(1+i, centers, crop, GT)\n",
    "\n",
    "# Single image result (index is added manually -- corresponding to calculated constants)\n",
    "\n",
    "# Imp note .... Desirable results may not be obtained in the first or second run ..... try running 3-4 times.\n",
    "# if output remains unchanged, it implies algorithm fails\n",
    "\n",
    "ReadImg(12, centers, crop, GT)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
